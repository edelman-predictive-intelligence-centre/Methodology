{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQh84cAQ53Nj"
   },
   "source": [
    "Input file is unlabeled 'united airline' data. The code is to generate topics by weeks using biterm topic modeling which is appropriate for short text. And then summarize similar topics in different weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZTowIhb-50hD"
   },
   "outputs": [],
   "source": [
    "!pip install biterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9LHR7RGzcMy"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.corpus\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def text_processing(text):\n",
    "    text = text.lower()\n",
    "    text = re.compile(r'https?://\\S+|www\\.\\S+').sub(r'', text)\n",
    "    text = text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "    text = \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "    text = \" \".join([stemmer.stem(WordNetLemmatizer().lemmatize(word, pos='v')) for word in str(text).split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JgWXikTfZMmv"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "  Process 'title' data, convert 'date' column to datetime type and sort by date column\n",
    "'''\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('news-articles-united-airline_social-animal_201601-201905.txt',sep='|')\n",
    "df_train['title'] = df_train['title'].apply(lambda x: text_processing(x))\n",
    "df_train['date'] = df_train['created_at'].apply(lambda x:datetime.datetime.strptime(x, '%Y/%m/%d %H:%M:%S').date())\n",
    "df_sorted = df_train.sort_values('date').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mD5l-7nHhVWP"
   },
   "outputs": [],
   "source": [
    "from biterm.cbtm import oBTM\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from biterm.utility import vec_to_biterms, topic_summuary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwHWplMp5PT3"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "  Generate topics in order by weeks using BTM. Generated data contains start date, end date of the week and top 10 topics in the week.\n",
    "'''\n",
    "date_started = df_sorted['date'][0]\n",
    "date_end = df_sorted['date'][len(df_sorted)-1] + datetime.timedelta(days=1)\n",
    "dict_timeline = {}\n",
    "dict_timeline['startdate'] = []\n",
    "dict_timeline['enddate'] = []\n",
    "dict_timeline['topics'] = []\n",
    "while date_started < date_end:\n",
    "    df_topic = df_sorted[(df_sorted['date']>=date_started) & (df_sorted['date']<(date_started+datetime.timedelta(days=7)))]  \n",
    "    vec = CountVectorizer(stop_words='english')\n",
    "    X = vec.fit_transform(df_topic['title']).toarray()\n",
    "    vocab = np.array(vec.get_feature_names())\n",
    "    biterms = vec_to_biterms(X)\n",
    "\n",
    "    btm = oBTM(num_topics=10, V=vocab)\n",
    "    for i in range(0,len(biterms),100):\n",
    "        biterms_chunk = biterms[i:i+100]\n",
    "        btm.fit(biterms_chunk,iterations=50)\n",
    "    topics = btm.transform(biterms)\n",
    "    topwords = topic_summuary(btm.phi_wz.T, X, vocab, 10,verbose=False)['top_words']\n",
    "    print('start date:{}, end date:{}'.format(date_started,(date_started+datetime.timedelta(days=6))))\n",
    "    print(topwords)\n",
    "    list_topic = []\n",
    "    for t in topwords:\n",
    "        list_topic.append(t.tolist())\n",
    "    dict_timeline['startdate'].append(date_started)\n",
    "    dict_timeline['enddate'].append(date_started+datetime.timedelta(days=6))\n",
    "    dict_timeline['topics'].append(list_topic)\n",
    "    date_started = date_started + datetime.timedelta(days=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n63CcyFC2xfF",
    "outputId": "e96374dd-6401-4940-9005-57896fb8e26b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 73 entries, 0 to 72\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   startdate  73 non-null     object\n",
      " 1   enddate    73 non-null     object\n",
      " 2   topics     73 non-null     object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_topics = pd.DataFrame.from_dict(dict_timeline,orient='index').T\n",
    "df_topics.info()\n",
    "df_topics.to_csv('topic_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9o7LuoqRT1Z4"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "  Separate topics in topic list of each week and generate new dataframe. Add new column 'in_timeline' for next step.\n",
    "'''\n",
    "dict_new = {}\n",
    "dict_new['startdate'] = []\n",
    "dict_new['enddate'] = []\n",
    "dict_new['topic'] = []\n",
    "for i,row in df_topics.iterrows():\n",
    "    for t in row['topics']:\n",
    "        dict_new['startdate'].append(row['startdate'])\n",
    "        dict_new['enddate'].append(row['enddate'])\n",
    "        dict_new['topic'].append(t)\n",
    "\n",
    "df_toptl = pd.DataFrame.from_dict(dict_new,orient='index').T\n",
    "df_toptl['in_timeline'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8dBrcGDT4eLg"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "  Summarize all the similar topics by timeline. If one topic have more than 3 words same as those in another topics, these 2 topics are\n",
    "  identified as similar topics. After one topic is identified as similar topic to the previous one, it would be annotated as 'in_timeline'\n",
    "  and ignored when loop to the row.\n",
    "'''\n",
    "dict_topic = {}\n",
    "dict_topic['startdate'] = []\n",
    "dict_topic['enddate'] = []\n",
    "dict_topic['words'] = []\n",
    "dict_topic['topic'] = []\n",
    "\n",
    "i_t = 0\n",
    "IGNORE = ['unit','airlin','flight','plane','man','woman']\n",
    "for index in range(len(df_toptl)):\n",
    "    if df_toptl.loc[index]['in_timeline'] == 0:\n",
    "        topic = df_toptl.loc[index]['topic']\n",
    "        i_t += 1\n",
    "        dict_topic['startdate'].append(df_toptl.loc[index]['startdate'])\n",
    "        dict_topic['enddate'].append(df_toptl.loc[index]['enddate'])\n",
    "        dict_topic['words'].append(topic)\n",
    "        dict_topic['topic'].append(i_t)\n",
    "        for i_2,row_2 in df_toptl[df_toptl['startdate']>df_toptl.loc[index]['startdate']].iterrows():\n",
    "            topic_2 = row_2['topic']\n",
    "            word_count = 0\n",
    "            for word in topic:\n",
    "                if word not in IGNORE and word in topic_2:\n",
    "                    word_count += 1\n",
    "            if word_count >= 3:\n",
    "                dict_topic['startdate'].append(row_2['startdate'])\n",
    "                dict_topic['enddate'].append(row_2['enddate'])\n",
    "                dict_topic['words'].append(topic_2)\n",
    "                dict_topic['topic'].append(i_t)\n",
    "                df_toptl.loc[i_2,'in_timeline']  = 1\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ndsOters5bo-"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "  Only retrieve topic groups which have more than 2 topics in timeline.\n",
    "'''\n",
    "df_similartopics  = pd.DataFrame.from_dict(dict_topic,orient='index').T\n",
    "df_final = df_similartopics[df_similartopics.groupby(['topic'])['words'].transform('count') > 2]\n",
    "df_final.to_csv('final_topics.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "biterm.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
