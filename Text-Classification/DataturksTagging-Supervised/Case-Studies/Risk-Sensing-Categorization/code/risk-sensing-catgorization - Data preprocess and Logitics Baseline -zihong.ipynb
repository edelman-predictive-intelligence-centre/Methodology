{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is the codebook about 1 VS N labelled classification.\n",
    "Before modeling, we preprocess the data, which included drop unimportant risk types and drop duplicated news.\n",
    "The methods we used: Logistics, new feature(word count) and oversample.    \n",
    "Finally, the logitics without word count perform best, which obtain acc 0.86 and F1 score (0.91 and 0.66).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# open a file, where you stored the pickled data\n",
    "file = open('traintest_2019NOV_RS_UNITED.pkl', 'rb')\n",
    "\n",
    "# dump information to that file\n",
    "data = pickle.load(file)\n",
    "\n",
    "# close the file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['risk_cat'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "risk_cat\n",
       "Not Risk                        1271\n",
       "Others                           927\n",
       "Employee Misconduct              763\n",
       "Service Failure/Availability     746\n",
       "Changes in Company Policy        422\n",
       "Product Availability             330\n",
       "Product Failure                  244\n",
       "Macro Environment                196\n",
       "Industry Adjacent                153\n",
       "Invalid                          134\n",
       "Customer Misconduct               92\n",
       "Confrontation                     91\n",
       "Organizational Reform             90\n",
       "Financial                         60\n",
       "Media Blunder                     39\n",
       "Auxiliary                         37\n",
       "Natural Disaster                  35\n",
       "Technological Infrastructure      13\n",
       "Malevolence                        1\n",
       "Name: contents, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('risk_cat')['contents'].count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the aboved results, there are 19 risk type in totall, we need to further filter the types.  \n",
    "After grouping and count the amount of data in each type.  \n",
    "We decde to drop malevolence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.loc[data['risk_cat']!='Malevolence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In case of some news have duplicated contents. We decide to check and delete duplicate content.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from simhash import Simhash, SimhashIndex\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 276 duplicated list of articles.\n",
      "Out of them, 157 list of articles has consistent labels while 119 does not.\n",
      "156 inconsistent articles are removed\n"
     ]
    }
   ],
   "source": [
    "def check_duplicates(index, objs):\n",
    "    dups_ = []\n",
    "    for i in range(len(objs)):\n",
    "        dups = index.get_near_dups(objs[i][1])\n",
    "        if len(dups) > 1:\n",
    "            dups.sort()\n",
    "            dups_.append(tuple(dups))\n",
    "    return list(set(dups_))\n",
    "\n",
    "def deep_unique(li):\n",
    "    using_ = li.copy()\n",
    "    for i in range(len(li)):\n",
    "        cur_tup = li[i]\n",
    "        l = len(cur_tup)\n",
    "        for j in li[i+1:]:\n",
    "            for k in cur_tup:\n",
    "                if k in j and l < len(j):\n",
    "                    if cur_tup in using_:\n",
    "                        using_.remove(cur_tup)\n",
    "                    break\n",
    "    return using_\n",
    "\n",
    "def check_label(i1, i2):\n",
    "    return df['risk_cat'].iloc[i1] == df['risk_cat'].iloc[i2]\n",
    "\n",
    "content = df.contents.to_numpy()\n",
    "objs = []\n",
    "for k in range(len(content)):\n",
    "    objs.append((str(k), Simhash(content[k])))\n",
    "    \n",
    "index = SimhashIndex(objs, k=3)\n",
    "dups = check_duplicates(index, objs)\n",
    "dups_unique = deep_unique(dups)\n",
    "\n",
    "consistent = []\n",
    "inconsistent = []\n",
    "\n",
    "for dups in dups_unique:\n",
    "    flag = True\n",
    "    for j in range(len(dups)):\n",
    "        if j + 1 < len(dups) and not check_label(int(dups[j]),int(dups[j+1])):\n",
    "            flag = False\n",
    "            break\n",
    "    if flag:\n",
    "        consistent.append(dups)\n",
    "    else:\n",
    "        inconsistent.append(dups)\n",
    "\n",
    "print(f\"There are {len(dups_unique)} duplicated list of articles.\")\n",
    "print(f\"Out of them, {len(consistent)} list of articles has consistent labels while {len(inconsistent)} does not.\")\n",
    "\n",
    "from collections import Counter\n",
    "flat_inconsistent = []\n",
    "revise_labels = {}\n",
    "for item in inconsistent:\n",
    "    if len(item)==2:\n",
    "        flat_inconsistent.extend(list(item))\n",
    "    else:\n",
    "        labels = [df['risk_cat'].iloc[int(i)] for i in item ]\n",
    "        temp = Counter(labels).most_common()[0]\n",
    "        if temp[1]/len(labels)>0.5:\n",
    "            for i in item:\n",
    "                revise_labels[i] = temp[0]\n",
    "        else:\n",
    "            flat_inconsistent.extend(list(item)) \n",
    "flat_inconsistent = [ item  for item in flat_inconsistent if int(item) not in revise_labels]\n",
    "remove_inconsistent = [item  for item in df.index if str(item) not in flat_inconsistent]\n",
    "\n",
    "df_prep1 = df.loc[remove_inconsistent,:]\n",
    "print(\"%s inconsistent articles are removed\"%len(flat_inconsistent))\n",
    "\n",
    "#flat_inconsistent = [ item  for item in flat_inconsistent if item not in revise_labels]\n",
    "#remove_inconsistent = [item  for item in df.index if str(item) not in flat_inconsistent]\n",
    "\n",
    "#df_prep1 = df.iloc[remove_inconsistent,:]\n",
    "#print(\"%s inconsistent articles are removed\"%len(flat_inconsistent))\n",
    "\n",
    "#for key,item in revise_labels.items():\n",
    "   # df_prep1.loc[int(key)]['risk_cat'] = item\n",
    "#print(\"After remove and relabel risk category, there are %s news\" %len(df_prep1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep1 = df_prep1.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "risk_cat\n",
       "Not Risk                        1244\n",
       "Others                           893\n",
       "Employee Misconduct              737\n",
       "Service Failure/Availability     730\n",
       "Changes in Company Policy        412\n",
       "Product Availability             323\n",
       "Product Failure                  243\n",
       "Macro Environment                188\n",
       "Industry Adjacent                149\n",
       "Invalid                          133\n",
       "Customer Misconduct               92\n",
       "Confrontation                     90\n",
       "Organizational Reform             84\n",
       "Financial                         59\n",
       "Media Blunder                     38\n",
       "Auxiliary                         35\n",
       "Natural Disaster                  34\n",
       "Technological Infrastructure      13\n",
       "Name: contents, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prep1.groupby('risk_cat')['contents'].count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5234"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1244+893+737+730+412+323+243+188+149+133+92+90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9273564847625797"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5234/data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We do not want to keep all categories, so we only remain majority categories, aka top 12 categories.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep2 = df_prep1.loc[df_prep1['risk_cat'].isin(['Macro Environment', 'Product Availability','Not Risk', \n",
    "                                                 'Industry Adjacent','Invalid',  'Product Failure',\n",
    "                                                'Service Failure/Availability', 'Others', 'Changes in Company Policy',\n",
    "                                                 'Employee Misconduct','Customer Misconduct', 'Confrontation']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "def tokenize_lemmatize(doc):\n",
    "    doc = re.sub(\"[\\xa0]+\",\"\",doc)\n",
    "    words = [v.lower() for v in word_tokenize(doc) if v.isalpha() or v.isdigit()]\n",
    "    words = [w for w  in words if w not in stopWords] \n",
    "    words = [''.join((c for c in unicodedata.normalize('NFD', w) if unicodedata.category(c) != 'Mn')) for w in words]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    words = [\"#number\" if w.isdigit() else w for w in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = df_prep2['contents'].values\n",
    "content = [tokenize_lemmatize(doc) for doc in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The orignal vacabuary size is 32380\n",
      "The refined vacabuary size is 11072\n"
     ]
    }
   ],
   "source": [
    "org_vacab  = Counter([v for item in content for v in item]).most_common() \n",
    "print(\"The orignal vacabuary size is %s\" %len(org_vacab))\n",
    "vocab_ = [item_value[0] for item_value in org_vacab if item_value[1]>5]\n",
    "print(\"The refined vacabuary size is %s\" %len(vocab_))\n",
    "content = [\" \".join([v for v in item if v in vocab_]) for item in content ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep2 = df_prep2.drop(columns=['level_0','index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep2['contents'] = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep2.to_csv('df_prep2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep3 = df_prep2.copy()\n",
    "l1 = []\n",
    "for i in range(df_prep3.shape[0]):\n",
    "    if df_prep3['risk_cat'][i]=='Not Risk':\n",
    "        l1.append(1)\n",
    "    else:\n",
    "        l1.append(0)\n",
    "df_prep3['Not Risk'] = l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(df_prep3.contents,df_prep3['Not Risk'],\n",
    "                                                                    test_size = 0.2, random_state = 42) \n",
    "tfidf1 = TfidfVectorizer() \n",
    "tfidf1.fit(x_train)  \n",
    "x_train= tfidf1.transform(x_train) \n",
    "x_test = tfidf1.transform(x_test) \n",
    "df_prep3['word_count'] = [len(item.split(\" \")) for item in df_prep3['contents']]\n",
    "x_train_wc =  hstack([x_train,np.array([df_prep3.iloc[y_train.index.values]['word_count']]).T])\n",
    "x_test_wc =  hstack([x_test,np.array([df_prep3.iloc[y_test.index.values]['word_count']]).T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8605539637058262\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91       803\n",
      "           1       0.76      0.58      0.66       244\n",
      "\n",
      "    accuracy                           0.86      1047\n",
      "   macro avg       0.82      0.76      0.79      1047\n",
      "weighted avg       0.85      0.86      0.85      1047\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\E055761\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "lr1.fit(x_train_wc, y_train)\n",
    "y_pred_log = lr1.predict_proba(x_test_wc)\n",
    "\n",
    "\n",
    "y_pred_log = [np.argmax(line) for line in y_pred_log]\n",
    "print('accuracy %s' % accuracy_score(y_pred_log, y_test))\n",
    "print(classification_report(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OVR classification   \n",
    "without wordcount + logistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8643744030563515\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92       803\n",
      "           1       0.80      0.56      0.66       244\n",
      "\n",
      "    accuracy                           0.86      1047\n",
      "   macro avg       0.84      0.76      0.79      1047\n",
      "weighted avg       0.86      0.86      0.86      1047\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr2 = LogisticRegression(random_state=123,penalty='l2', solver='lbfgs')\n",
    "lr2.fit(x_train, y_train)\n",
    "y_pred_log = lr2.predict_proba(x_test)\n",
    "\n",
    "\n",
    "y_pred_log = [np.argmax(line) for line in y_pred_log]\n",
    "print('accuracy %s' % accuracy_score(y_pred_log, y_test))\n",
    "print(classification_report(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OVR classification   \n",
    "oversample + wordcount + log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.828080229226361\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.85      0.88       803\n",
      "           1       0.61      0.75      0.67       244\n",
      "\n",
      "    accuracy                           0.83      1047\n",
      "   macro avg       0.76      0.80      0.78      1047\n",
      "weighted avg       0.85      0.83      0.83      1047\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\E055761\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#oversample wordcount\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "os = RandomOverSampler(sampling_strategy='minority')\n",
    "x_train_wc_os, y_train_os = os.fit_sample(x_train_wc,y_train)\n",
    "\n",
    "\n",
    "lr3 = LogisticRegression(random_state=123,penalty='l2', solver='lbfgs')\n",
    "lr3.fit(x_train_wc_os, y_train_os)\n",
    "y_pred_log = lr3.predict_proba(x_test_wc)\n",
    "\n",
    "\n",
    "y_pred_log = [np.argmax(line) for line in y_pred_log]\n",
    "print('accuracy %s' % accuracy_score(y_pred_log, y_test))\n",
    "print(classification_report(y_test, y_pred_log))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OVR classification   \n",
    "oversample + without word count + log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8271251193887297\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.85      0.88       803\n",
      "           1       0.60      0.75      0.67       244\n",
      "\n",
      "    accuracy                           0.83      1047\n",
      "   macro avg       0.76      0.80      0.78      1047\n",
      "weighted avg       0.84      0.83      0.83      1047\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#oversample withoutcount\n",
    "os = RandomOverSampler(sampling_strategy='minority')\n",
    "x_train_os, y_train_os = os.fit_sample(x_train,y_train)\n",
    "\n",
    "\n",
    "lr4 = LogisticRegression(random_state=123,penalty='l2', solver='lbfgs')\n",
    "lr4.fit(x_train_os, y_train_os)\n",
    "y_pred_log = lr4.predict_proba(x_test)\n",
    "\n",
    "\n",
    "y_pred_log = [np.argmax(line) for line in y_pred_log]\n",
    "print('accuracy %s' % accuracy_score(y_pred_log, y_test))\n",
    "print(classification_report(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
