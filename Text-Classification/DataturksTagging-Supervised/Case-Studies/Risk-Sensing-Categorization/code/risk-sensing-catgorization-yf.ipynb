{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../input/traintest_2019NOV_RS_UNITED.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5644 entries, 0 to 5643\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   article_id       5644 non-null   object\n",
      " 1   link             5644 non-null   object\n",
      " 2   headline         5644 non-null   object\n",
      " 3   contents         5644 non-null   object\n",
      " 4   annotation       5644 non-null   object\n",
      " 5   done_by          5644 non-null   object\n",
      " 6   risk_cat         5644 non-null   object\n",
      " 7   annotation_note  5644 non-null   object\n",
      "dtypes: object(8)\n",
      "memory usage: 396.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.corpus\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "import re\n",
    "import string\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "def text_processing(text):\n",
    "    text = text.lower()\n",
    "    text = re.compile(r'https?://\\S+|www\\.\\S+').sub(r'', text)\n",
    "    text = text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "    text = \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "    text = \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'] = df['risk_cat'].apply(lambda x: x if x in ['Not Risk','Others'] else 'Risk')\n",
    "df['cat_code_full'] = df['risk_cat'].astype('category').cat.codes\n",
    "df['cat_code'] = df['category'].astype('category').cat.codes\n",
    "df['cleaned_content'] = df['contents'].apply(lambda x:text_processing(x))\n",
    "df['cleaned_headline'] = df['headline'].apply(lambda x:text_processing(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['cleaned_content','cleaned_headline']], df['cat_code'], \n",
    "                                                    test_size=0.2,shuffle=True,random_state = 42)\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(df[['cleaned_content','cleaned_headline']], df['cat_code_full'], \n",
    "                                                    test_size=0.2,shuffle=True,random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_content = TfidfVectorizer()\n",
    "tfidf_content.fit(X_train['cleaned_content'])  \n",
    "content_train = tfidf_content.transform(X_train['cleaned_content']).toarray()\n",
    "content_test = tfidf_content.transform(X_test['cleaned_content']).toarray()\n",
    "\n",
    "tfidf_headline = TfidfVectorizer()\n",
    "tfidf_headline.fit(X_train['cleaned_headline'])  \n",
    "headline_train = tfidf_headline.transform(X_train['cleaned_headline']).toarray()\n",
    "headline_test = tfidf_headline.transform(X_test['cleaned_headline']).toarray()\n",
    "\n",
    "X_train =  np.hstack((content_train,headline_train))\n",
    "X_test =  np.hstack((content_test,headline_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_content_full = TfidfVectorizer()\n",
    "tfidf_content_full.fit(X_train_full['cleaned_content'])  \n",
    "content_train_full = tfidf_content_full.transform(X_train_full['cleaned_content']).toarray()\n",
    "content_test_full = tfidf_content_full.transform(X_test_full['cleaned_content']).toarray()\n",
    "\n",
    "tfidf_headline_full = TfidfVectorizer()\n",
    "tfidf_headline_full.fit(X_train_full['cleaned_headline'])  \n",
    "headline_train_full = tfidf_headline_full.transform(X_train_full['cleaned_headline']).toarray()\n",
    "headline_test_full = tfidf_headline_full.transform(X_test_full['cleaned_headline']).toarray()\n",
    "\n",
    "X_train_full =  np.hstack((content_train_full,headline_train_full))\n",
    "X_test_full =  np.hstack((content_test_full,headline_test_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=123)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "lr1 = linear_model.LogisticRegression(random_state=123,penalty='l2', solver='lbfgs')\n",
    "lr1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report(3 categories):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.64      0.70       253\n",
      "           1       0.65      0.44      0.53       189\n",
      "           2       0.77      0.89      0.83       687\n",
      "\n",
      "    accuracy                           0.76      1129\n",
      "   macro avg       0.73      0.66      0.68      1129\n",
      "weighted avg       0.75      0.76      0.75      1129\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report(3 categories):\\n',classification_report(y_test, lr1.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\E055762\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report(full categories):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         7\n",
      "           1       0.84      0.70      0.76       103\n",
      "           2       0.67      0.15      0.25        13\n",
      "           3       1.00      0.61      0.76        18\n",
      "           4       0.68      0.57      0.62       164\n",
      "           5       0.71      0.42      0.53        12\n",
      "           6       0.67      0.38      0.49        26\n",
      "           7       1.00      0.04      0.07        26\n",
      "           8       0.68      0.34      0.45        44\n",
      "           9       0.00      0.00      0.00         1\n",
      "          10       1.00      0.20      0.33         5\n",
      "          11       1.00      0.43      0.60         7\n",
      "          12       0.61      0.82      0.70       253\n",
      "          13       0.92      0.58      0.71        19\n",
      "          14       0.52      0.70      0.60       189\n",
      "          15       0.77      0.65      0.71        63\n",
      "          16       0.80      0.90      0.85        40\n",
      "          17       0.67      0.65      0.66       136\n",
      "          18       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.65      1129\n",
      "   macro avg       0.66      0.43      0.48      1129\n",
      "weighted avg       0.67      0.65      0.63      1129\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\E055762\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "lr2 = linear_model.LogisticRegression(random_state=123,penalty='l2', solver='lbfgs')\n",
    "lr2.fit(X_train_full, y_train_full)\n",
    "print('Classification Report(full categories):\\n',classification_report(y_test_full, lr2.predict(X_test_full)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\E055762\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:39:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Classification Report(3 categories):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.60      0.68       253\n",
      "           1       0.59      0.43      0.50       189\n",
      "           2       0.77      0.89      0.83       687\n",
      "\n",
      "    accuracy                           0.75      1129\n",
      "   macro avg       0.71      0.64      0.67      1129\n",
      "weighted avg       0.74      0.75      0.74      1129\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb1 = xgb.XGBClassifier(n_estimators = 300, learning_rate = 0.1,random_state = 42)\n",
    "xgb1.fit(X_train,y_train)\n",
    "print('Classification Report(3 categories):\\n',classification_report(y_test, xgb1.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:16:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Classification Report(full categories):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.14      0.25         7\n",
      "           1       0.82      0.67      0.74       103\n",
      "           2       0.33      0.15      0.21        13\n",
      "           3       0.93      0.72      0.81        18\n",
      "           4       0.63      0.60      0.61       164\n",
      "           5       0.71      0.42      0.53        12\n",
      "           6       0.65      0.50      0.57        26\n",
      "           7       0.60      0.23      0.33        26\n",
      "           8       0.50      0.18      0.27        44\n",
      "           9       0.00      0.00      0.00         1\n",
      "          10       0.50      0.20      0.29         5\n",
      "          11       0.67      0.57      0.62         7\n",
      "          12       0.61      0.77      0.68       253\n",
      "          13       0.91      0.53      0.67        19\n",
      "          14       0.50      0.67      0.57       189\n",
      "          15       0.69      0.70      0.69        63\n",
      "          16       0.82      0.93      0.87        40\n",
      "          17       0.70      0.60      0.65       136\n",
      "          18       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.63      1129\n",
      "   macro avg       0.61      0.45      0.49      1129\n",
      "weighted avg       0.64      0.63      0.62      1129\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\E055762\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "xgb2 = xgb.XGBClassifier(n_estimators = 200, learning_rate = 0.1,random_state = 42)\n",
    "xgb2.fit(X_train_full,y_train_full)\n",
    "print('Classification Report(full categories):\\n',classification_report(y_test_full, xgb2.predict(X_test_full)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
