{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from simhash import Simhash, SimhashIndex\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2561 observations in the raw dataset\n",
      "There are 2557 observations with 1 C1 label\n",
      "There are 4 observations with 2 C1 label\n"
     ]
    }
   ],
   "source": [
    "#Import data labelled using Social Animal\n",
    "df_original = pd.read_csv('data/input/dt_sa_merge_20200609.txt',sep =\"|\").dropna(subset=['C1'])\n",
    "print(\"There are %s observations in the raw dataset\" % len(df_original))\n",
    "\n",
    "#Articles with only one C1 label\n",
    "df = df_original[[ True if len( re.findall(r'C1',str(item)))==1 else False \n",
    "                  for item in df_original['C1'] ]].reset_index(drop=True)\n",
    "df['C1'] = df['C1'].replace({'C1-Law / Policy Enforcement / Prevention':\"C1-Merged\",\n",
    "                             'C1-Study / Report / Commentary':\"C1-Merged\"}) \n",
    "print(\"There are %s observations with 1 C1 label\" % len(df))\n",
    "\n",
    "#Articles with only one C2 label\n",
    "df_2 = df_original[[ True if len( re.findall(r'C1',str(item)))==2 else False for item in df_original['C1'] ]]\n",
    "print(\"There are %s observations with 2 C1 label\" % len(df_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data Preparation\n",
    "\n",
    "### Remove Mislabeled Articles\n",
    "Articles with nearly duplicated contents with different C1 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 141 duplicated list of articles.\n",
      "Out of them, 95 list of articles has consistent labels while 46 does not.\n",
      "69 inconsistent articles are removed\n",
      "69 inconsistent articles are removed\n",
      "After remove and relabel C1 category, there are 2490 articles\n"
     ]
    }
   ],
   "source": [
    "def check_duplicates(index, objs):\n",
    "    dups_ = []\n",
    "    for i in range(len(objs)):\n",
    "        dups = index.get_near_dups(objs[i][1])\n",
    "        if len(dups) > 1:\n",
    "            dups.sort()\n",
    "            dups_.append(tuple(dups))\n",
    "    return list(set(dups_))\n",
    "\n",
    "def deep_unique(li):\n",
    "    using_ = li.copy()\n",
    "    for i in range(len(li)):\n",
    "        cur_tup = li[i]\n",
    "        l = len(cur_tup)\n",
    "        for j in li[i+1:]:\n",
    "            for k in cur_tup:\n",
    "                if k in j and l < len(j):\n",
    "                    if cur_tup in using_:\n",
    "                        using_.remove(cur_tup)\n",
    "                    break\n",
    "    return using_\n",
    "\n",
    "def check_label(i1, i2):\n",
    "    return df.C1.iloc[i1] == df.C1.iloc[i2]\n",
    "\n",
    "content = df.content.to_numpy()\n",
    "objs = []\n",
    "for k in range(len(content)):\n",
    "    objs.append((str(k), Simhash(content[k])))\n",
    "    \n",
    "index = SimhashIndex(objs, k=3)\n",
    "dups = check_duplicates(index, objs)\n",
    "dups_unique = deep_unique(dups)\n",
    "\n",
    "consistent = []\n",
    "inconsistent = []\n",
    "\n",
    "for dups in dups_unique:\n",
    "    flag = True\n",
    "    for j in range(len(dups)):\n",
    "        if j + 1 < len(dups) and not check_label(int(dups[j]),int(dups[j+1])):\n",
    "            flag = False\n",
    "            break\n",
    "    if flag:\n",
    "        consistent.append(dups)\n",
    "    else:\n",
    "        inconsistent.append(dups)\n",
    "\n",
    "print(f\"There are {len(dups_unique)} duplicated list of articles.\")\n",
    "print(f\"Out of them, {len(consistent)} list of articles has consistent labels while {len(inconsistent)} does not.\")\n",
    "\n",
    "from collections import Counter\n",
    "flat_inconsistent = []\n",
    "revise_labels = {}\n",
    "for item in inconsistent:\n",
    "    if len(item)==2:\n",
    "        flat_inconsistent.extend(list(item))\n",
    "    else:\n",
    "        labels = [df.C1.iloc[int(i)] for i in item ]\n",
    "        temp = Counter(labels).most_common()[0]\n",
    "        if temp[1]/len(labels)>0.5:\n",
    "            for i in item:\n",
    "                revise_labels[i] = temp[0]\n",
    "        else:\n",
    "            flat_inconsistent.extend(list(item)) \n",
    "flat_inconsistent = [ item  for item in flat_inconsistent if int(item) not in revise_labels]\n",
    "remove_inconsistent = [item  for item in df.index if str(item) not in flat_inconsistent]\n",
    "\n",
    "df_prep1 = df.loc[remove_inconsistent,:]\n",
    "print(\"%s inconsistent articles are removed\"%len(flat_inconsistent))\n",
    "\n",
    "flat_inconsistent = [ item  for item in flat_inconsistent if item not in revise_labels]\n",
    "remove_inconsistent = [item  for item in df.index if str(item) not in flat_inconsistent]\n",
    "\n",
    "df_prep1 = df.iloc[remove_inconsistent,:]\n",
    "print(\"%s inconsistent articles are removed\"%len(flat_inconsistent))\n",
    "\n",
    "for key,item in revise_labels.items():\n",
    "    df_prep1.loc[int(key)]['C1'] = item\n",
    "print(\"After remove and relabel C1 category, there are %s articles\" %len(df_prep1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Irrelevant Cateogry Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After remove irrelevant Cateogry, there are 2365 articles\n"
     ]
    }
   ],
   "source": [
    "df_prep2 =  df_prep1[[True if item in ['C1-Other', 'C1-Merged', 'C1-Trafficking Case / Story'] else False \n",
    "                      for item in df_prep1['C1']]].reset_index(drop = True)\n",
    "print(\"After remove irrelevant Cateogry, there are %s articles\" %len(df_prep2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Relabeled Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep2['C1-Merged'] = ['C1-Merged' ==item for item in df_prep2['C1'].values]\n",
    "df_prep2['C1-Other'] = ['C1-Other'==item for item in df_prep2['C1'].values]\n",
    "df_prep2['C1-Trafficking Case/Story'] = ['C1-Trafficking Case / Story'==item for item in df_prep2['C1'].values]\n",
    "\n",
    "df_newtag = pd.read_json('data/input/sa_human_trafficking_20191217_20200315_20200709_dowjones retagging.json', lines=True)\n",
    "df_newtag['url'] = [item.split(\"\\n\\n\\n\")[0][1:] for item in df_newtag['content']]\n",
    "\n",
    "df_newtag['C1-Merged'] = [True if 'C1- Merged' in item['labels'] else False for item in df_newtag['annotation'].values]\n",
    "df_newtag['C1-Other'] = [True if 'C1- Other' in item['labels'] else False for item in df_newtag['annotation'].values]\n",
    "df_newtag['C1-Trafficking Case/Story'] = [True if 'C1- Trafficking Case/Story' in item['labels']  \n",
    "                                          else False for item in df_newtag['annotation'].values]\n",
    "temp1 = pd.merge(df_prep2.drop(['annotation','C1-Merged','C1-Other','C1-Trafficking Case/Story'],axis = 1),\n",
    "                    df_newtag[['annotation', 'C1-Merged','C1-Other','C1-Trafficking Case/Story','url']],on='url') \n",
    "temp2 = df_prep2[[True if item not in temp1['url'].values else False for item in df_prep2['url']]]\n",
    "df_prep3 = temp2.append(temp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove wordpress content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After remove wordpress content, there are 2340 articles\n",
      "C1-Merged                    1002\n",
      "C1-Other                      471\n",
      "C1-Trafficking Case/Story     905\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_prep4 = df_prep3[[False if type(item) == str and 'wordpress.com' in item else True for item in df_prep3.url.values ]]\n",
    "print(\"After remove wordpress content, there are %s articles\" %len(df_prep4))\n",
    "print(df_prep4.sum()[['C1-Merged','C1-Other','C1-Trafficking Case/Story']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_lemmatize(doc):\n",
    "    doc = re.sub(\"[\\xa0]+\",\"\",doc)\n",
    "    words = [v.lower() for v in word_tokenize(doc) if v.isalpha() or v.isdigit()]\n",
    "    words = [w for w  in words if w not in stopWords] \n",
    "    words = [''.join((c for c in unicodedata.normalize('NFD', w) if unicodedata.category(c) != 'Mn')) for w in words]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    words = [\"#number\" if w.isdigit() else w for w in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = df_prep4['content'].values\n",
    "content = [tokenize_lemmatize(doc) for doc in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The orignal vacabuary size is 33060\n",
      "The refined vacabuary size is 10416\n"
     ]
    }
   ],
   "source": [
    "org_vacab  = Counter([v for item in content for v in item]).most_common() \n",
    "print(\"The orignal vacabuary size is %s\" %len(org_vacab))\n",
    "vocab_ = [item_value[0] for item_value in org_vacab if item_value[1]>5]\n",
    "print(\"The refined vacabuary size is %s\" %len(vocab_))\n",
    "content = [\" \".join([v for v in item if v in vocab_]) for item in content ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_prep4.reset_index(drop=True)\n",
    "df_new['content'] = content "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Train & Test\n",
    "\n",
    "TFIDF + Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(X_test, Y_test, svr):\n",
    "    predictY = svr.predict(X_test)\n",
    "    result = sum(predictY == Y_test)\n",
    "\n",
    "    return float(result)/len(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trafficking Case / Story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=123, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix, hstack\n",
    "df_new['word_count'] = [len(item.split(\" \")) for item in df_new['content']]\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(df_new.content,df_new['C1-Trafficking Case/Story'],\n",
    "                                                                    test_size = 0.2, random_state = 42) \n",
    "tfidf1 = TfidfVectorizer() \n",
    "tfidf1.fit(x_train)  \n",
    "x_train= tfidf1.transform(x_train) \n",
    "x_test = tfidf1.transform(x_test) \n",
    "x_train =  hstack([x_train,np.array([df_new.iloc[y_train.index.values]['word_count']]).T])\n",
    "x_test =  hstack([x_test,np.array([df_new.iloc[y_test.index.values]['word_count']]).T])\n",
    "lr1 = linear_model.LogisticRegression(random_state=123,penalty='l2', solver='lbfgs')\n",
    "lr1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test - Social Animal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.87      0.96      0.91       279\n",
      "        True       0.93      0.79      0.85       189\n",
      "\n",
      "   micro avg       0.89      0.89      0.89       468\n",
      "   macro avg       0.90      0.87      0.88       468\n",
      "weighted avg       0.89      0.89      0.89       468\n",
      "\n",
      "        Predicted_0  Predicted_1\n",
      "True_0          267           12\n",
      "True_1           40          149\n",
      "The model accuracy is 0.89\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, lr1.predict(x_test)))\n",
    "cm = pd.DataFrame(confusion_matrix(y_test, lr1.predict(x_test) ) )\n",
    "cm.columns =['Predicted_0','Predicted_1'] \n",
    "cm.index =  ['True_0','True_1']\n",
    "print(cm)\n",
    "accuracy_test2 = model_test(x_test, y_test, lr1)\n",
    "print(\"The model accuracy is %.2f\" % accuracy_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test - Dow Jones Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.91      0.96      0.93        74\n",
      "        True       0.86      0.73      0.79        26\n",
      "\n",
      "   micro avg       0.90      0.90      0.90       100\n",
      "   macro avg       0.89      0.85      0.86       100\n",
      "weighted avg       0.90      0.90      0.90       100\n",
      "\n",
      "        Predicted_0  Predicted_1\n",
      "True_0           71            3\n",
      "True_1            7           19\n",
      "The model accuracy is 0.90\n"
     ]
    }
   ],
   "source": [
    "df_dj_hist = pd.read_json('data/input/dowjone_dt_sample_20200714.json', lines=True)\n",
    "df_dj_hist['C1-Merged'] = [True if 'C1- Merged' in item['labels'] else False for item in df_dj_hist['annotation'].values]\n",
    "df_dj_hist['C1-Other'] = [True if 'C1- Other' in item['labels'] else False for item in df_dj_hist['annotation'].values]\n",
    "df_dj_hist['C1-Trafficking Case/Story'] = [True if 'C1- Trafficking Case/Story' in item['labels']  \n",
    "                                          else False for item in df_dj_hist['annotation'].values]\n",
    "\n",
    "content_test = df_dj_hist['content'].values\n",
    "content_test = [tokenize_lemmatize(doc) for doc in content_test]\n",
    "content_test = [\" \".join([v for v in item if v in vocab_]) for item in content_test]\n",
    "df_dj_hist['content'] = content_test\n",
    "df_dj_hist['word_count'] = [len(item.split(\" \")) for item in df_dj_hist['content']]\n",
    "x_val = tfidf1.transform(content_test) \n",
    "y_val = df_dj_hist['C1-Trafficking Case/Story'].values\n",
    "\n",
    "x_val =  hstack([x_val,np.array( [df_dj_hist['word_count']]).T])\n",
    "print(classification_report(y_val, lr1.predict(x_val)))\n",
    "\n",
    "cm = pd.DataFrame(confusion_matrix(y_val, lr1.predict(x_val) ) )\n",
    "cm.columns =['Predicted_0','Predicted_1'] \n",
    "cm.index =  ['True_0','True_1']\n",
    "print(cm)\n",
    "accuracy_test2 = model_test(x_val, y_val, lr1)\n",
    "print(\"The model accuracy is %.2f\" % accuracy_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test - Dow Jones Phase I data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.89      0.93      0.91        61\n",
      "        True       0.89      0.82      0.85        39\n",
      "\n",
      "   micro avg       0.89      0.89      0.89       100\n",
      "   macro avg       0.89      0.88      0.88       100\n",
      "weighted avg       0.89      0.89      0.89       100\n",
      "\n",
      "        Predicted_0  Predicted_1\n",
      "True_0           57            4\n",
      "True_1            7           32\n",
      "The model accuracy is 0.89\n"
     ]
    }
   ],
   "source": [
    "df_dj_p1 = pd.read_json('data/input/phase_1_dt_sample_20200720.json', lines=True)\n",
    "df_dj_p1['C1-Merged'] = [True if 'C1- Merged' in item['labels'] else False for item in df_dj_p1['annotation'].values]\n",
    "df_dj_p1['C1-Other'] = [True if 'C1- Other' in item['labels'] else False for item in df_dj_p1['annotation'].values]\n",
    "df_dj_p1['C1-Trafficking Case/Story'] = [True if 'C1- Trafficking Case/Story' in item['labels']  \n",
    "                                          else False for item in df_dj_p1['annotation'].values]\n",
    "\n",
    "content_test = df_dj_p1['content'].values\n",
    "content_test = [tokenize_lemmatize(doc) for doc in content_test]\n",
    "content_test = [\" \".join([v for v in item if v in vocab_]) for item in content_test]\n",
    "df_dj_p1['content'] = content_test\n",
    "df_dj_p1['word_count'] = [len(item.split(\" \")) for item in df_dj_p1['content']]\n",
    "x_val = tfidf1.transform(content_test) \n",
    "y_val = df_dj_p1['C1-Trafficking Case/Story'].values\n",
    "\n",
    "x_val =  hstack([x_val,np.array( [df_dj_p1['word_count']]).T])\n",
    "print(classification_report(y_val, lr1.predict(x_val)))\n",
    "\n",
    "cm = pd.DataFrame(confusion_matrix(y_val, lr1.predict(x_val) ) )\n",
    "cm.columns =['Predicted_0','Predicted_1'] \n",
    "cm.index =  ['True_0','True_1']\n",
    "print(cm)\n",
    "accuracy_test2 = model_test(x_val, y_val, lr1)\n",
    "print(\"The model accuracy is %.2f\" % accuracy_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Law policy prevention & study report commentary\n",
    "#### Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=123, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix, hstack\n",
    "df_new['word_count'] = [len(item.split(\" \")) for item in df_new['content']]\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(df_new.content,df_new['C1-Merged'],\n",
    "                                                                    test_size = 0.2, random_state = 42) \n",
    "tfidf2 = TfidfVectorizer() \n",
    "x_train = tfidf2.fit_transform(x_train)  \n",
    "x_test = tfidf2.transform(x_test) \n",
    "x_train =  hstack([x_train,np.array([df_new.iloc[y_train.index.values]['word_count']]).T])\n",
    "x_test =  hstack([x_test,np.array([df_new.iloc[y_test.index.values]['word_count']]).T])\n",
    "lr2 = linear_model.LogisticRegression(random_state=123,penalty='l2', solver='lbfgs')\n",
    "lr2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test - Social Animal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.86      0.88      0.87       278\n",
      "        True       0.81      0.78      0.80       190\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       468\n",
      "   macro avg       0.84      0.83      0.83       468\n",
      "weighted avg       0.84      0.84      0.84       468\n",
      "\n",
      "        Predicted_0  Predicted_1\n",
      "True_0          244           34\n",
      "True_1           41          149\n",
      "The model accuracy is 0.84\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, lr2.predict(x_test)))\n",
    "cm = pd.DataFrame(confusion_matrix(y_test, lr2.predict(x_test) ) )\n",
    "cm.columns =['Predicted_0','Predicted_1'] \n",
    "cm.index =  ['True_0','True_1']\n",
    "print(cm)\n",
    "accuracy_test2 = model_test(x_test, y_test, lr2)\n",
    "print(\"The model accuracy is %.2f\" % accuracy_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test - Dow Jones Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.59      0.89      0.71        61\n",
      "        True       0.22      0.05      0.08        39\n",
      "\n",
      "   micro avg       0.56      0.56      0.56       100\n",
      "   macro avg       0.41      0.47      0.40       100\n",
      "weighted avg       0.45      0.56      0.47       100\n",
      "\n",
      "        Predicted_0  Predicted_1\n",
      "True_0           54            7\n",
      "True_1           37            2\n",
      "\n",
      " The model accuracy is 0.56\n"
     ]
    }
   ],
   "source": [
    "df_dj_hist = pd.read_json('data/input/dowjone_dt_sample_20200714.json', lines=True)\n",
    "df_dj_hist['C1-Merged'] = [True if 'C1- Merged' in item['labels'] else False for item in df_dj_hist['annotation'].values]\n",
    "df_dj_hist['C1-Other'] = [True if 'C1- Other' in item['labels'] else False for item in df_dj_hist['annotation'].values]\n",
    "df_dj_hist['C1-Trafficking Case/Story'] = [True if 'C1- Trafficking Case/Story' in item['labels']  \n",
    "                                          else False for item in df_dj_hist['annotation'].values]\n",
    "df_dj_hist['content'] = content_test\n",
    "df_dj_hist['word_count'] = [len(item.split(\" \")) for item in df_dj_hist['content']]\n",
    "\n",
    "content_test = df_dj_hist['content'].values\n",
    "content_test = [tokenize_lemmatize(doc) for doc in content_test]\n",
    "content_test = [\" \".join([v for v in item if v in vocab_]) for item in content_test]\n",
    "x_val = tfidf2.transform(content_test) \n",
    "y_val = df_dj_hist['C1-Merged'].values\n",
    "\n",
    "x_val =  hstack([x_val,np.array( [df_dj_hist['word_count']]).T])\n",
    "print(classification_report(y_val, lr2.predict(x_val)))\n",
    "\n",
    "cm = pd.DataFrame(confusion_matrix(y_val, lr2.predict(x_val) ) )\n",
    "cm.columns =['Predicted_0','Predicted_1'] \n",
    "cm.index =  ['True_0','True_1']\n",
    "print(cm)\n",
    "accuracy_test2 = model_test(x_val, y_val, lr2)\n",
    "print(\"\\n The model accuracy is %.2f\" % accuracy_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test - Dow Jones Phase I data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.62      0.97      0.75        58\n",
      "        True       0.78      0.17      0.27        42\n",
      "\n",
      "   micro avg       0.63      0.63      0.63       100\n",
      "   macro avg       0.70      0.57      0.51       100\n",
      "weighted avg       0.68      0.63      0.55       100\n",
      "\n",
      "        Predicted_0  Predicted_1\n",
      "True_0           56            2\n",
      "True_1           35            7\n",
      "The model accuracy is 0.63\n"
     ]
    }
   ],
   "source": [
    "df_dj_p1 = pd.read_json('data/input/phase_1_dt_sample_20200720.json', lines=True)\n",
    "df_dj_p1['C1-Merged'] = [True if 'C1- Merged' in item['labels'] else False for item in df_dj_p1['annotation'].values]\n",
    "df_dj_p1['C1-Other'] = [True if 'C1- Other' in item['labels'] else False for item in df_dj_p1['annotation'].values]\n",
    "df_dj_p1['C1-Trafficking Case/Story'] = [True if 'C1- Trafficking Case/Story' in item['labels']  \n",
    "                                          else False for item in df_dj_p1['annotation'].values]\n",
    "\n",
    "content_test = df_dj_p1['content'].values\n",
    "content_test = [tokenize_lemmatize(doc) for doc in content_test]\n",
    "content_test = [\" \".join([v for v in item if v in vocab_]) for item in content_test]\n",
    "df_dj_p1['content'] = content_test\n",
    "df_dj_p1['word_count'] = [len(item.split(\" \")) for item in df_dj_p1['content']]\n",
    "x_val = tfidf2.transform(content_test) \n",
    "y_val = df_dj_p1['C1-Merged'].values\n",
    "\n",
    "x_val =  hstack([x_val,np.array( [df_dj_p1['word_count']]).T])\n",
    "print(classification_report(y_val, lr2.predict(x_val)))\n",
    "\n",
    "cm = pd.DataFrame(confusion_matrix(y_val, lr2.predict(x_val) ) )\n",
    "cm.columns =['Predicted_0','Predicted_1'] \n",
    "cm.index =  ['True_0','True_1']\n",
    "print(cm)\n",
    "accuracy_test2 = model_test(x_val, y_val, lr2)\n",
    "print(\"The model accuracy is %.2f\" % accuracy_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lr1, open(f\"model/lr1_trafficking_case_story.pkl\",\"wb\"))\n",
    "pickle.dump(tfidf1, open(f\"model/tfidf1_trafficking_case_story.pkl\",\"wb\"))\n",
    "pickle.dump(lr2, open(f\"model/lr2_trafficking_merged.pkl\",\"wb\"))\n",
    "pickle.dump(tfidf2, open(f\"model/tfidf2_trafficking_merged.pkl\",\"wb\"))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
