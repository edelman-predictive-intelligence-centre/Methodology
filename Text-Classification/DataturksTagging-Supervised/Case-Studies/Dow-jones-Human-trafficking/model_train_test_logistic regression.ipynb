{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from simhash import Simhash, SimhashIndex\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2561 observations in the raw dataset\n",
      "There are 2557 observations with one C1 label\n",
      "There are 4 observations with two C1 label\n"
     ]
    }
   ],
   "source": [
    "#Import data labelled using Social Animal\n",
    "df_original = pd.read_csv('data/input/dt_sa_merge_20200609.txt',sep =\"|\").dropna(subset=['C1'])\n",
    "print(\"There are %s observations in the raw dataset\" % len(df_original))\n",
    "\n",
    "#Articles with only one C1 label\n",
    "df = df_original[[ True if len( re.findall(r'C1',str(item)))==1 else False \n",
    "                  for item in df_original['C1'] ]].reset_index(drop=True)\n",
    "\n",
    "#Merge \"C1-Law / Policy Enforcement / Prevention\"  and \"C1-Study / Report / Commentary\" under one category \"C1-merged\"\n",
    "#As we found there are hard to distinguish in the labelling process and also with classification model.\n",
    "df['C1'] = df['C1'].replace({'C1-Law / Policy Enforcement / Prevention':\"C1-Merged\",\n",
    "                             'C1-Study / Report / Commentary':\"C1-Merged\"}) \n",
    "print(\"There are %s observations with one C1 label\" % len(df))\n",
    "\n",
    "#Articles with only two C1 label\n",
    "df_2 = df_original[[ True if len( re.findall(r'C1',str(item)))==2 else False for item in df_original['C1'] ]]\n",
    "print(\"There are %s observations with two C1 label\" % len(df_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data Preparation\n",
    "\n",
    "### Remove Mislabeled Articles\n",
    "Articles with nearly duplicated contents with different C1 labels are removed. Nearly duplocated Content are Identified through Simhash Algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 146 duplicated list of articles.\n",
      "Out of them, 100 list of articles has consistent labels while 46 does not.\n",
      "69 inconsistent articles are removed\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "511",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 511",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-ac1080d03c38>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrevise_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[0mdf_prep1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'C1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"After remove and relabel C1 category, there are %s articles\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_prep1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1423\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1424\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1426\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1848\u001b[0m         \u001b[1;31m# fall thru to straight lookup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1850\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1851\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[1;34m(self, label, axis)\u001b[0m\n\u001b[0;32m    158\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"no slices here, handle elsewhere\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mxs\u001b[1;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[0;32m   3735\u001b[0m             \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc_level\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_level\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3736\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3737\u001b[1;33m             \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3738\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3739\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2897\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2899\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 511"
     ]
    }
   ],
   "source": [
    "def check_duplicates(index, objs):\n",
    "    dups_ = []\n",
    "    for i in range(len(objs)):\n",
    "        dups = index.get_near_dups(objs[i][1])\n",
    "        if len(dups) > 1:\n",
    "            dups.sort()\n",
    "            dups_.append(tuple(dups))\n",
    "    return list(set(dups_))\n",
    "\n",
    "def deep_unique(li):\n",
    "    using_ = li.copy()\n",
    "    for i in range(len(li)):\n",
    "        cur_tup = li[i]\n",
    "        l = len(cur_tup)\n",
    "        for j in li[i+1:]:\n",
    "            for k in cur_tup:\n",
    "                if k in j and l < len(j):\n",
    "                    if cur_tup in using_:\n",
    "                        using_.remove(cur_tup)\n",
    "                    break\n",
    "    return using_\n",
    "\n",
    "def check_label(i1, i2):\n",
    "    return df.C1.iloc[i1] == df.C1.iloc[i2]\n",
    "\n",
    "content = df.content.to_numpy()\n",
    "objs = []\n",
    "for k in range(len(content)):\n",
    "    objs.append((str(k), Simhash(content[k])))\n",
    "    \n",
    "index = SimhashIndex(objs, k=3)\n",
    "dups = check_duplicates(index, objs)\n",
    "dups_unique = deep_unique(dups)\n",
    "\n",
    "consistent = []\n",
    "inconsistent = []\n",
    "\n",
    "for dups in dups_unique:\n",
    "    flag = True\n",
    "    for j in range(len(dups)):\n",
    "        if j + 1 < len(dups) and not check_label(int(dups[j]),int(dups[j+1])):\n",
    "            flag = False\n",
    "            break\n",
    "    if flag:\n",
    "        consistent.append(dups)\n",
    "    else:\n",
    "        inconsistent.append(dups)\n",
    "\n",
    "print(f\"There are {len(dups_unique)} duplicated list of articles.\")\n",
    "print(f\"Out of them, {len(consistent)} list of articles has consistent labels while {len(inconsistent)} does not.\")\n",
    "\n",
    "from collections import Counter\n",
    "flat_inconsistent = []\n",
    "revise_labels = {}\n",
    "for item in inconsistent:\n",
    "    if len(item)==2:\n",
    "        flat_inconsistent.extend(list(item))\n",
    "    else:\n",
    "        labels = [df.C1.iloc[int(i)] for i in item ]\n",
    "        temp = Counter(labels).most_common()[0]\n",
    "        if temp[1]/len(labels)>0.5:\n",
    "            for i in item:\n",
    "                revise_labels[i] = temp[0]\n",
    "        else:\n",
    "            flat_inconsistent.extend(list(item)) \n",
    "flat_inconsistent = [ item  for item in flat_inconsistent if int(item) not in revise_labels]\n",
    "remove_inconsistent = [item  for item in df.index if str(item) not in flat_inconsistent]\n",
    "\n",
    "df_prep1 = df.loc[remove_inconsistent,:]\n",
    "print(\"%s inconsistent articles are removed\"%len(flat_inconsistent))\n",
    "\n",
    "for key,item in revise_labels.items():\n",
    "    df_prep1.loc[int(key)]['C1'] = item\n",
    "print(\"After remove and relabel C1 category, there are %s articles\" %len(df_prep1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Irrelevant Cateogry Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep2 =  df_prep1[[True if item in ['C1-Other', 'C1-Merged', 'C1-Trafficking Case / Story'] else False \n",
    "                      for item in df_prep1['C1']]].reset_index(drop = True)\n",
    "print(\"After remove irrelevant Cateogry, there are %s articles\" %len(df_prep2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Relabeled Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep2['C1-Merged'] = ['C1-Merged' ==item for item in df_prep2['C1'].values]\n",
    "df_prep2['C1-Other'] = ['C1-Other'==item for item in df_prep2['C1'].values]\n",
    "df_prep2['C1-Trafficking Case/Story'] = ['C1-Trafficking Case / Story'==item for item in df_prep2['C1'].values]\n",
    "\n",
    "df_newtag = pd.read_json('data/input/sa_human_trafficking_20191217_20200315_20200709_dowjones retagging.json', lines=True)\n",
    "df_newtag['url'] = [item.split(\"\\n\\n\\n\")[0][1:] for item in df_newtag['content']]\n",
    "\n",
    "df_newtag['C1-Merged'] = [True if 'C1- Merged' in item['labels'] else False for item in df_newtag['annotation'].values]\n",
    "df_newtag['C1-Other'] = [True if 'C1- Other' in item['labels'] else False for item in df_newtag['annotation'].values]\n",
    "df_newtag['C1-Trafficking Case/Story'] = [True if 'C1- Trafficking Case/Story' in item['labels']  \n",
    "                                          else False for item in df_newtag['annotation'].values]\n",
    "temp1 = pd.merge(df_prep2.drop(['annotation','C1-Merged','C1-Other','C1-Trafficking Case/Story'],axis = 1),\n",
    "                    df_newtag[['annotation', 'C1-Merged','C1-Other','C1-Trafficking Case/Story','url']],on='url') \n",
    "temp2 = df_prep2[[True if item not in temp1['url'].values else False for item in df_prep2['url']]]\n",
    "df_prep3 = temp2.append(temp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove wordpress content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep4 = df_prep3[[False if type(item) == str and 'wordpress.com' in item else True for item in df_prep3.url.values ]]\n",
    "print(\"After remove wordpress content, there are %s articles\" %len(df_prep4))\n",
    "print(df_prep4.sum()[['C1-Merged','C1-Other','C1-Trafficking Case/Story']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_lemmatize(doc):\n",
    "    doc = re.sub(\"[\\xa0]+\",\"\",doc)\n",
    "    words = [v.lower() for v in word_tokenize(doc) if v.isalpha() or v.isdigit()]\n",
    "    words = [w for w  in words if w not in stopWords] \n",
    "    words = [''.join((c for c in unicodedata.normalize('NFD', w) if unicodedata.category(c) != 'Mn')) for w in words]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    words = [\"#number\" if w.isdigit() else w for w in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = df_prep4['content'].values\n",
    "content = [tokenize_lemmatize(doc) for doc in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_vacab  = Counter([v for item in content for v in item]).most_common() \n",
    "print(\"The orignal vacabuary size is %s\" %len(org_vacab))\n",
    "vocab_ = [item_value[0] for item_value in org_vacab if item_value[1]>5]\n",
    "print(\"The refined vacabuary size is %s\" %len(vocab_))\n",
    "content = [\" \".join([v for v in item if v in vocab_]) for item in content ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_prep4.reset_index(drop=True)\n",
    "df_new['content'] = content "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Train & Test\n",
    "\n",
    "TFIDF + Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(X_test, Y_test, svr):\n",
    "    predictY = svr.predict(X_test)\n",
    "    result = sum(predictY == Y_test)\n",
    "\n",
    "    return float(result)/len(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trafficking Case / Story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, hstack\n",
    "df_new['word_count'] = [len(item.split(\" \")) for item in df_new['content']]\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(df_new.content,df_new['C1-Trafficking Case/Story'],\n",
    "                                                                    test_size = 0.2, random_state = 42) \n",
    "tfidf1 = TfidfVectorizer() \n",
    "tfidf1.fit(x_train)  \n",
    "x_train= tfidf1.transform(x_train) \n",
    "x_test = tfidf1.transform(x_test) \n",
    "x_train =  hstack([x_train,np.array([df_new.iloc[y_train.index.values]['word_count']]).T])\n",
    "x_test =  hstack([x_test,np.array([df_new.iloc[y_test.index.values]['word_count']]).T])\n",
    "lr1 = linear_model.LogisticRegression(random_state=123,penalty='l2', solver='lbfgs')\n",
    "lr1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test - Social Animal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, lr1.predict(x_test)))\n",
    "cm = pd.DataFrame(confusion_matrix(y_test, lr1.predict(x_test) ) )\n",
    "cm.columns =['Predicted_0','Predicted_1'] \n",
    "cm.index =  ['True_0','True_1']\n",
    "print(cm)\n",
    "accuracy_test2 = model_test(x_test, y_test, lr1)\n",
    "print(\"The model accuracy is %.2f\" % accuracy_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test - Dow Jones Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dj_hist = pd.read_json('data/input/dowjone_dt_sample_20200714.json', lines=True)\n",
    "df_dj_hist['C1-Merged'] = [True if 'C1- Merged' in item['labels'] else False for item in df_dj_hist['annotation'].values]\n",
    "df_dj_hist['C1-Other'] = [True if 'C1- Other' in item['labels'] else False for item in df_dj_hist['annotation'].values]\n",
    "df_dj_hist['C1-Trafficking Case/Story'] = [True if 'C1- Trafficking Case/Story' in item['labels']  \n",
    "                                          else False for item in df_dj_hist['annotation'].values]\n",
    "\n",
    "content_test = df_dj_hist['content'].values\n",
    "content_test = [tokenize_lemmatize(doc) for doc in content_test]\n",
    "content_test = [\" \".join([v for v in item if v in vocab_]) for item in content_test]\n",
    "df_dj_hist['content'] = content_test\n",
    "df_dj_hist['word_count'] = [len(item.split(\" \")) for item in df_dj_hist['content']]\n",
    "x_val = tfidf1.transform(content_test) \n",
    "y_val = df_dj_hist['C1-Trafficking Case/Story'].values\n",
    "\n",
    "x_val =  hstack([x_val,np.array( [df_dj_hist['word_count']]).T])\n",
    "print(classification_report(y_val, lr1.predict(x_val)))\n",
    "\n",
    "cm = pd.DataFrame(confusion_matrix(y_val, lr1.predict(x_val) ) )\n",
    "cm.columns =['Predicted_0','Predicted_1'] \n",
    "cm.index =  ['True_0','True_1']\n",
    "print(cm)\n",
    "accuracy_test2 = model_test(x_val, y_val, lr1)\n",
    "print(\"The model accuracy is %.2f\" % accuracy_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test - Dow Jones Phase I data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dj_p1 = pd.read_json('data/input/phase_1_dt_sample_20200720.json', lines=True)\n",
    "df_dj_p1['C1-Merged'] = [True if 'C1- Merged' in item['labels'] else False for item in df_dj_p1['annotation'].values]\n",
    "df_dj_p1['C1-Other'] = [True if 'C1- Other' in item['labels'] else False for item in df_dj_p1['annotation'].values]\n",
    "df_dj_p1['C1-Trafficking Case/Story'] = [True if 'C1- Trafficking Case/Story' in item['labels']  \n",
    "                                          else False for item in df_dj_p1['annotation'].values]\n",
    "\n",
    "content_test = df_dj_p1['content'].values\n",
    "content_test = [tokenize_lemmatize(doc) for doc in content_test]\n",
    "content_test = [\" \".join([v for v in item if v in vocab_]) for item in content_test]\n",
    "df_dj_p1['content'] = content_test\n",
    "df_dj_p1['word_count'] = [len(item.split(\" \")) for item in df_dj_p1['content']]\n",
    "x_val = tfidf1.transform(content_test) \n",
    "y_val = df_dj_p1['C1-Trafficking Case/Story'].values\n",
    "\n",
    "x_val =  hstack([x_val,np.array( [df_dj_p1['word_count']]).T])\n",
    "print(classification_report(y_val, lr1.predict(x_val)))\n",
    "\n",
    "cm = pd.DataFrame(confusion_matrix(y_val, lr1.predict(x_val) ) )\n",
    "cm.columns =['Predicted_0','Predicted_1'] \n",
    "cm.index =  ['True_0','True_1']\n",
    "print(cm)\n",
    "accuracy_test2 = model_test(x_val, y_val, lr1)\n",
    "print(\"The model accuracy is %.2f\" % accuracy_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Law policy prevention & study report commentary\n",
    "#### Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, hstack\n",
    "df_new['word_count'] = [len(item.split(\" \")) for item in df_new['content']]\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(df_new.content,df_new['C1-Merged'],\n",
    "                                                                    test_size = 0.2, random_state = 42) \n",
    "tfidf2 = TfidfVectorizer() \n",
    "x_train = tfidf2.fit_transform(x_train)  \n",
    "x_test = tfidf2.transform(x_test) \n",
    "x_train =  hstack([x_train,np.array([df_new.iloc[y_train.index.values]['word_count']]).T])\n",
    "x_test =  hstack([x_test,np.array([df_new.iloc[y_test.index.values]['word_count']]).T])\n",
    "lr2 = linear_model.LogisticRegression(random_state=123,penalty='l2', solver='lbfgs')\n",
    "lr2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test - Social Animal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, lr2.predict(x_test)))\n",
    "cm = pd.DataFrame(confusion_matrix(y_test, lr2.predict(x_test) ) )\n",
    "cm.columns =['Predicted_0','Predicted_1'] \n",
    "cm.index =  ['True_0','True_1']\n",
    "print(cm)\n",
    "accuracy_test2 = model_test(x_test, y_test, lr2)\n",
    "print(\"The model accuracy is %.2f\" % accuracy_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test - Dow Jones Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dj_hist = pd.read_json('data/input/dowjone_dt_sample_20200714.json', lines=True)\n",
    "df_dj_hist['C1-Merged'] = [True if 'C1- Merged' in item['labels'] else False for item in df_dj_hist['annotation'].values]\n",
    "df_dj_hist['C1-Other'] = [True if 'C1- Other' in item['labels'] else False for item in df_dj_hist['annotation'].values]\n",
    "df_dj_hist['C1-Trafficking Case/Story'] = [True if 'C1- Trafficking Case/Story' in item['labels']  \n",
    "                                          else False for item in df_dj_hist['annotation'].values]\n",
    "df_dj_hist['content'] = content_test\n",
    "df_dj_hist['word_count'] = [len(item.split(\" \")) for item in df_dj_hist['content']]\n",
    "\n",
    "content_test = df_dj_hist['content'].values\n",
    "content_test = [tokenize_lemmatize(doc) for doc in content_test]\n",
    "content_test = [\" \".join([v for v in item if v in vocab_]) for item in content_test]\n",
    "x_val = tfidf2.transform(content_test) \n",
    "y_val = df_dj_hist['C1-Merged'].values\n",
    "\n",
    "x_val =  hstack([x_val,np.array( [df_dj_hist['word_count']]).T])\n",
    "print(classification_report(y_val, lr2.predict(x_val)))\n",
    "\n",
    "cm = pd.DataFrame(confusion_matrix(y_val, lr2.predict(x_val) ) )\n",
    "cm.columns =['Predicted_0','Predicted_1'] \n",
    "cm.index =  ['True_0','True_1']\n",
    "print(cm)\n",
    "accuracy_test2 = model_test(x_val, y_val, lr2)\n",
    "print(\"\\n The model accuracy is %.2f\" % accuracy_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test - Dow Jones Phase I data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dj_p1 = pd.read_json('data/input/phase_1_dt_sample_20200720.json', lines=True)\n",
    "df_dj_p1['C1-Merged'] = [True if 'C1- Merged' in item['labels'] else False for item in df_dj_p1['annotation'].values]\n",
    "df_dj_p1['C1-Other'] = [True if 'C1- Other' in item['labels'] else False for item in df_dj_p1['annotation'].values]\n",
    "df_dj_p1['C1-Trafficking Case/Story'] = [True if 'C1- Trafficking Case/Story' in item['labels']  \n",
    "                                          else False for item in df_dj_p1['annotation'].values]\n",
    "\n",
    "content_test = df_dj_p1['content'].values\n",
    "content_test = [tokenize_lemmatize(doc) for doc in content_test]\n",
    "content_test = [\" \".join([v for v in item if v in vocab_]) for item in content_test]\n",
    "df_dj_p1['content'] = content_test\n",
    "df_dj_p1['word_count'] = [len(item.split(\" \")) for item in df_dj_p1['content']]\n",
    "x_val = tfidf2.transform(content_test) \n",
    "y_val = df_dj_p1['C1-Merged'].values\n",
    "\n",
    "x_val =  hstack([x_val,np.array( [df_dj_p1['word_count']]).T])\n",
    "print(classification_report(y_val, lr2.predict(x_val)))\n",
    "\n",
    "cm = pd.DataFrame(confusion_matrix(y_val, lr2.predict(x_val) ) )\n",
    "cm.columns =['Predicted_0','Predicted_1'] \n",
    "cm.index =  ['True_0','True_1']\n",
    "print(cm)\n",
    "accuracy_test2 = model_test(x_val, y_val, lr2)\n",
    "print(\"The model accuracy is %.2f\" % accuracy_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lr1, open(f\"model/lr1_trafficking_case_story.pkl\",\"wb\"))\n",
    "pickle.dump(tfidf1, open(f\"model/tfidf1_trafficking_case_story.pkl\",\"wb\"))\n",
    "pickle.dump(lr2, open(f\"model/lr2_trafficking_merged.pkl\",\"wb\"))\n",
    "pickle.dump(tfidf2, open(f\"model/tfidf2_trafficking_merged.pkl\",\"wb\"))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
